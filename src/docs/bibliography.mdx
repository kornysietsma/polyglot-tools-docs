---
title: 'Bibliography'
---
import CitationAnchor from '../components/CitationAnchor.js'
import Citation from '../components/Citation.js'

**This page is a work in progress**

This is both research I've looked at, and a handful of my notes - it's not really a formal bibliography, but I'm trying to roughly follow standard references for clarity.

## A note on research limitations

It's fascinating reading a lot of this research.  An awful lot of it draws conclusions, or _implies_ conclusions, based on one particular kind of codebase or kind of organisation; the trouble is, software development is extremely diverse, in kinds of organisation, software architectures, and software development practices.  And it seems a lot of academic research is done without much effort to think about these differences - partly because it's actually quite hard to get your hands on a diverse range of commercial software sources!  And quite a bit of research is 20+ years old, from giant organisations, so it's unlikely to talk about microservices or TDD all that much.

I'll try to insert brief notes in this bibliography to clarify the kinds of code and organisation they looked at.  All the data is useful - but the context is important.

## Books

### Your Code as a Crime Scene <CitationAnchor>Tor15</CitationAnchor>

Adam Tornhill. Your Code as a Crime Scene. The Pragmatic Bookshelf, Raleigh, NC, 2015. [GoodReads](https://www.goodreads.com/book/show/23627482-your-code-as-a-crime-scene)

> A great overview of code-maat and other related code investigation techniques.  This was the book that started me looking at alternatives to traditional code metrics!

## Papers

### Bell, Ostrand and Weyuker

These folks have a bunch of papers about subsequent research, which I'm clumping together despite the alphabetical ordering being a bit strange (as they take turns as first author?)

#### Looking for bugs in all the right places. <CitationAnchor>BOW06</CitationAnchor>

Bell, R. M., Ostrand, T. J., & Weyuker, E. J. (2006, July). Looking for bugs in all the right places. In Proceedings of the 2006 international symposium on Software testing and analysis (pp. 61-72).

> Notes that most of their metrics are not much better than just looking at lines of code.

#### Using developer information as a factor for fault prediction.  <CitationAnchor>WOB07</CitationAnchor>

Weyuker, E. J., Ostrand, T. J., & Bell, R. M. (2007, May). Using developer information as a factor for fault prediction. In Third International Workshop on Predictor Models in Software Engineering (PROMISE'07: ICSE Workshops 2007) (pp. 8-8). IEEE.

> This one adds developer info such as number of devs in recent release, number of new devs, cumulative devs during all prior releases

#### Automating algorithms for the identification of fault-prone files <CitationAnchor>OWB07</CitationAnchor>

Ostrand, T. J., Weyuker, E. J., & Bell, R. M. (2007, July). Automating algorithms for the identification of fault-prone files. In Proceedings of the 2007 international symposium on Software testing and analysis (pp. 219-227).

#### Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models <CitationAnchor>WOB08</CitationAnchor>

Weyuker, E. J., Ostrand, T. J., & Bell, R. M. (2008). Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models. Empirical Software Engineering, 13(5), 539-559.

> Adding developer counts to their analysis. Useful but see notes above.

#### Comparing the effectiveness of several modeling methods for fault prediction. <CitationAnchor>WOB10</CitationAnchor>

Weyuker, E. J., Ostrand, T. J., & Bell, R. M. (2010). Comparing the effectiveness of several modeling methods for fault prediction. Empirical Software Engineering, 15(3), 277-295.

> mostly this is about different statistical modeling approaches.

#### Does measuring code change improve fault prediction? <CitationAnchor>BOW11</CitationAnchor>

Bell, R. M., Ostrand, T. J., & Weyuker, E. J. (2011, September). Does measuring code change improve fault prediction?. In Proceedings of the 7th International Conference on Predictive Models in Software Engineering (pp. 1-8).

> looking at including code change info to their modeling

### Don't touch my code! Examining the effects of ownership on software quality. <CitationAnchor>BNMGD11</CitationAnchor>

Bird, C., Nagappan, N., Murphy, B., Gall, H., & Devanbu, P. (2011, September). Don't touch my code! Examining the effects of ownership on software quality. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering (pp. 4-14).

> Microsoft research, largely on Windows Vista and Windows 7 code
>
> Seems a bit mixed between "code ownership" and "allowing 'low expertise' developers to make unsupervised changes" - maybe it's just my biases, but the conclusion seems to be that it's good to have single points of failure owning critical codebases, rather than working out the cultural / communication problems...

### A metric for software readability <CitationAnchor>BW08</CitationAnchor>

Buse, R. P., & Weimer, W. R. (2008, July). A metric for software readability. In Proceedings of the 2008 international symposium on Software testing and analysis (pp. 121-130).

> An attempt to build up metrics around "readability" - whatever that means.
> Has a broad range of sources used

### Code Smells Quantification: A Case Study On Large Open Source Research Codebase. <CitationAnchor>Cha19</CitationAnchor>

Chauhan, Swapnil Singh, "Code Smells Quantification: A Case Study On Large Open Source Research Codebase" (2019). Open Access Theses & Dissertations. 50. https://scholarworks.utep.edu/open_etd/50

> Interesting as it has a comparison of the state of an academic codebase (QGis) vs a commercial open-source codebase (tensor-flow) and discusses at least two styles of coding.

### Fractal figures: Visualizing development effort for cvs entities <CitationAnchor>DLG05</CitationAnchor>

D'Ambros, M., Lanza, M., & Gall, H. (2005, September). Fractal figures: Visualizing development effort for cvs entities. In 3rd IEEE International Workshop on Visualizing Software for Understanding and Analysis (pp. 1-6). IEEE.

> This looks at ways to visualise, and categorise, developer effort - identifying code that is clustered around a few key owners, vs spread out across many
> Not just visualisation, it gives an algorithm for producing a metric
> TODO - it'd be interesting to add this to the Polyglot Code Explorer.

### On the relationship between change coupling and software defects <CitationAnchor>DLR09</CitationAnchor>

D'Ambros, M., Lanza, M., & Robbes, R. (2009, October). On the relationship between change coupling and software defects. In 2009 16th Working Conference on Reverse Engineering (pp. 135-144). IEEE.

> Quite detailed calculations showing correlation between tightly coupled code and defect levels
>
> This is mostly around monolithic applications so uses specific commits not time windows - I probably need to track per-commit coupling as well, my coupling code is still under active development!

### Does code decay? assessing the evidence from change management data. <CitationAnchor>EGKMM01</CitationAnchor>

Eick, S. G., Graves, T. L., Karr, A. F., Marron, J. S., & Mockus, A. (2001). Does code decay? assessing the evidence from change management data. IEEE Transactions on Software Engineering, 27(1), 1-12.

> Quite old - the paper is from 2001, the code is older - fascinating for how giant long-lived projects operated in the 90s, and looking at ways to identify code decay. Even hints that "refactoring" might be of some value.

### Predicting fault incidence using software change history. <CitationAnchor>GKMS00</CitationAnchor>

Graves, T. L., Karr, A. F., Marron, J. S., & Siy, H. (2000). Predicting fault incidence using software change history. IEEE Transactions on software engineering, 26(7), 653-661.

> Quite old - 2000, code from 1997
>
> Concludes that older modules with large recent changes make for most faults.

### Predicting faults using the complexity of code changes <CitationAnchor>Has09</CitationAnchor>

Hassan, A. E. (2009, May). Predicting faults using the complexity of code changes. In 2009 IEEE 31st international conference on software engineering (pp. 78-88). IEEE.

> Looks very interesting, needs a more detailed read
>
> TODO: read this!

### Reading beside the lines: Indentation as a proxy for complexity metric. <CitationAnchor>HGH08</CitationAnchor>

Hindle, A., Godfrey, M. W., & Holt, R. C. (2008, June). Reading beside the lines: Indentation as a proxy for complexity metric. In 2008 16th IEEE International Conference on Program Comprehension (pp. 133-142). IEEE.

> This is the main reference for the indentation-as-complexity visualisation.
> Interesting that they indicate standard deviation is a good way to look at indentation

### An empirical study on the impact of duplicate code <CitationAnchor>HSSHK12</CitationAnchor>

Hotta, K., Sasaki, Y., Sano, Y., Higo, Y., & Kusumoto, S. (2012). An empirical study on the impact of duplicate code. Advances in Software Engineering, 2012.

> What it says on the tin - trying to identify whether duplicate code is as bad as everyone assumes

### Non-essential changes in version histories <CitationAnchor>KR11</CitationAnchor>

Kawrykow, D., & Robillard, M. P. (2011, May). Non-essential changes in version histories. In 2011 33rd International Conference on Software Engineering (ICSE) (pp. 351-360). IEEE.

> Improving change metrics by eliminating changes which are just trivial like indentation or variable renaming.

> Sadly this is completely dependent on language-specific parsing to identify type signatures, so not much use for me.

### Visualization-based analysis of quality for large-scale software systems. <CitationAnchor>LSP05</CitationAnchor>

Langelier, G., Sahraoui, H., & Poulin, P. (2005, November). Visualization-based analysis of quality for large-scale software systems. In Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering (pp. 214-223).

### Attitudes, beliefs and development data concerning agile software development practices <CitationAnchor>MHDT19</CitationAnchor>

Matthies, C., Huegle, J., Dürschmid, T., & Teusner, R. (2019, May). Attitudes, beliefs, and development data concerning agile software development practices. In 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET) (pp. 158-169). IEEE.

> Includes interesting stuff on detection of agile practices via code artifacts!

### Use of relative code churn measures to predict system defect density <CitationAnchor>NB05</CitationAnchor>

Nagappan, N., & Ball, T. (2005, May). Use of relative code churn measures to predict system defect density. In Proceedings of the 27th international conference on Software engineering (pp. 284-292).

> Instead of absolute churn, looks at churn relative to (say) lines of code, different relative time scales

> Windows Server 2003 codebase

### The influence of organizational structure on software quality <CitationAnchor>NMB08</CitationAnchor>

Nagappan, N., Murphy, B., & Basili, V. (2008, May). The influence of organizational structure on software quality. In 2008 ACM/IEEE 30th International Conference on Software Engineering (pp. 521-530). IEEE.

> Using metrics like employee churn, position of developers on org chart - very cool stuff
> They conclude it is more effective to use organisational metrics than code quality metrics!
> Contains a useful summary of prior research too
>
> TODO: read further

### Source code properties of defective infrastructure as code scripts. <CitationAnchor>RW19</CitationAnchor>

Rahman, A., & Williams, L. (2019). Source code properties of defective infrastructure as code scripts. Information and Software Technology, 112, 148-163.

> Interesting to look at Infrastructure as Code - a different kind of code.  Includes correlations with number of static strings in codebases!

### Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities. <CitationAnchor>SMWO10</CitationAnchor>

Shin, Y., Meneely, A., Williams, L., & Osborne, J. A. (2010). Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities. IEEE transactions on software engineering, 37(6), 772-787.

> Analysis of Firefox and the Linux kernel

> TODO: this needs a more detailed read

### Cross-project defect prediction: a large scale experiment on data vs. domain vs. process. <CitationAnchor>ZNGGM09</CitationAnchor>

Zimmermann, T., Nagappan, N., Gall, H., Giger, E., & Murphy, B. (2009, August). Cross-project defect prediction: a large scale experiment on data vs. domain vs. process. In Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering (pp. 91-100).

> Microsoft research, but not on just their code but others
>
> lots of cool metrics and whether they actually work across projects and orgs. For example IE vs Firefox.
>
> TODO: needs more detailed reading

## Still to find and read

"Code smells for multi-language systems" paper from europlop 19

"visual detection of design anomalies" - refers to <Citation to='LSP05'></Citation>
